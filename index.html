<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GLIGEN:Open-Set Grounded Text-to-Image Generation.">
  <meta name="keywords" content="Image Generation, Diffusion, Grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GLIGEN:Open-Set Grounded Text-to-Image Generation.</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  </head>
  <body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GLIGEN:</h1>
          <h2 class="title is-2 publication-title">Open-Set Grounded Text-to-Image Generation</h2>
          <div class="is-size-5">
            <span class="author-block">
                <a href="https://yuheng-li.github.io/" style="color:#f68946;font-weight:normal;">Yuheng Li</a>,                
            </span>
            <span class="author-block">
              <a href="https://hliu.cc/" style="color:#f68946;font-weight:normal;">Haotian Liu</a>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&hl=en/" style="color:#F2A900;font-weight:normal;">Qingyang Wu</a>,
            </span>
            <span class="author-block">
              <a href="https://pages.cs.wisc.edu/~fmu/" style="color:#f68946;font-weight:normal;">Fangzhou Mu</a>,
            </span>
            <span class="author-block">
              <a href="https://jwyang.github.io/" style="color:#008AD7;font-weight:normal;">Jianwei Yang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/" style="color:#008AD7;font-weight:normal;">Jianfeng Gao</a>,
            </span>  
            <br>
            <span class="author-block">
              <a href="https://chunyuan.li/" style="color:#008AD7;font-weight:normal;">Chunyuan Li<sup>*</sup></a>,
            </span>   
            <span class="author-block">
              <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae Lee<sup>*</sup></a>,
            </span>                                  
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of Wisconsin-Madison; </b></span>
            <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b>Columbia University; </span>
            <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b>Microsoft</span>
            <span class="author-block">&nbsp&nbsp<sup>*</sup>Equal Advising</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2301.07093" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/gligen/GLIGEN" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://aka.ms/gligen" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://youtu.be/-MCkU7IAGKs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="150%" src="images/teaser_v4.png">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>Figure 1. GLIGEN enables versatile grounding capabilities for a frozen text-to-image generation model.</b></p>
      </h2>
    </div>
  </div>
</section>


<section class="section"   style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large-scale <a href="https://github.com/CompVis/stable-diffusion">text-to-image diffusion models</a> have made
            amazing advances. However, the status quo is to use
            text input alone, which can impede controllability. In this
            work, we propose GLIGEN, <b>G</b>rounded-<b>L</b>anguage-to-<b>I</b>mage
            <b>Gen</b>eration, a novel approach that builds upon and extends
            the functionality of existing pre-trained text-to-image diffusion models 
            by enabling them to also be conditioned on
            <a href="https://github.com/microsoft/GLIP">grounding inputs</a>. <b>To preserve the vast concept knowledge
              of the pre-trained model, we freeze all of its weights and
              inject the grounding information into new trainable layers
              via a gated mechanism</b>. Our model achieves open-world
            grounded text2img generation with caption and bounding
            box condition inputs, and the grounding ability generalizes
            well to novel spatial configuration and concepts. GLIGENâ€™s
            zero-shot performance on COCO and LVIS outperforms that
            of existing supervised layout-to-image baselines by a large
            margin.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>





<section class="section"   style="background-color:#e4e4f781">
  <div class="columns is-centered has-text-centered">
      <div style="width:90%;">
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3"><a href="https://aka.ms/gligen" target="_blank"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5886/5886212.png"> Demo -- Try it out  </a> </h2> 
          </div>
        </div>
        <!-- Abstract. -->
        <div style="float:left; width:47.1%; border: 0px solid rgba(5, 130, 255, 0.534);">
          <h2 class="title is-5">I. Generated examples from GLIGEN Demo</h2>
          <div class="column is-five-fifths">
              <div class="columns is-centered">
                <img id="teaser" width="100%" src="images/same_box.gif">
              </div>
          </div>
        </div>
        <!--/ Abstract. -->

        <div style=" float:right; width:49.8%; border: 0px solid black;">
          <h2 class="title is-5">II. Demo Instruction</h2>
          <div class="column is-five-fifths">
            <div class="columns is-centered">
            <div class="publication-video">
              <iframe src="https://youtu.be/-MCkU7IAGKs" frameborder="1" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<!--               <iframe src="https://user-images.githubusercontent.com/6631389/211468127-60dd5c69-8db9-43d2-a45d-e97b336e5249.mp4" frameborder="1" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
            </div>
          </div>
          </div>
        </div>
      </div>  
    </div>     
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Model Designs: Efficient Training & Flexible Inference </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <div class="content has-text-justified">
              <ul>
                <li>GLIGEN is built upon existing pretrained diffusion models. whose original weights are <b>frozen</b> to preserve vast pre-trained knowledge.</li>
                <li>A new trainable Gated Self-Attention layer is added at each transformer block to absorb new grounding input.</li>
                <li>Each grounding token consists of two types of information: <b>semantic</b> of grounded entity (encoded text or image) and <b>spatial location</b> (encoded bounding box or keypoints).</li>
              </ul>
            </div>        
            <img id="model" width="50%" src="images/approach_v1.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 2. Gated Self-Attention is used to fuse new grounding tokens.</b></p>
            </h3>   


        </div>
  </div>
</section>



<section class="section"   style="background-color:#e4e4f781">
  <div class="columns is-centered has-text-centered">
      <div style="width:90%;">

        <!-- Abstract. -->
        <div style="float:left; width:47.1%; border: 0px solid black;">
          <h2 class="title is-5">I. Modulated Training</h2>
          <div class="content has-text-justified">
              Compared with other ways of using a pretrained diffusion model such as full-model finetuning, our newly added modulated layers are continual pre-trained on large grounding data (image-text-box) and is more cost-efficient. Just like Lego, one can plug and play different trained layers to enable different new capabilities.                 
          </div>  

          <div class="column is-five-fifths">
              <div class="columns is-centered">
                <img id="modulated_training" width="105%" src="images/approach_change.gif">
                
              </div>
          </div>
        </div>
        <!--/ Abstract. -->

        <div style=" float:right; width:47.8%; border: 0px solid black;">
          <h2 class="title is-5">II. Scheduled Sampling</h2>
          <div class="content has-text-justified">
              As a favorable property of our modulated training, GLIGEN supports scheduled sampling in the diffusion process for inference, where the model can dynamically choose to use grounding tokens (by adding the new layer) or original diffusion model with good prior (by kicking out the new layer), and thus balances generation quality and grounding ability.                   
          </div>  

          <div class="column is-five-fifths">
              <div class="columns is-centered">
                <img id="modulated_training" width="97%" src="images/approach_sampling.gif">
                
              </div>
          </div>
        </div>

      </div>  
    </div>     
</section>



<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/3515/3515174.png"> Results</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">


  <!-- Grounedtext2img. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Text Grounded T2I Generation (Bounding box)</h2>
      <img id="teaser" width="95%" src="images/groundtext2img.png">
      <h1>
        <p style="font-family:Times New Roman"><b>By exploiting knowledge of pretrained text2img model, GLIGEN can generate varieties of objects in given locations, it also supports varies of styles.</b>
      </h1>     
      <br>
      <img id="teaser" width="95%" src="images/gligen_vs_dalle_images.png">
      <h1>
        <p style="font-family:Times New Roman"><b>Compared with existing text2img models such as DALLE1 and DALLE2, GLIGEN enables the new capability to allow grounding instruction. The text prompt and DALLE generated images are from <a href='https://openai.com/dall-e-2/'>OpenAI Blog</a>.</b>
      </h1>       
    </div>
  </div>

  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Spatially counterfactual generation</h2>
      <img id="teaser" width="95%" src="images/counterfactual.png">
      <h1>
        <p style="font-family:Times New Roman"><b>By explicitly specifying object size and location, GLIGEN can generate spatially counterfactual results which are difficult to release through text2img model (e.g., Stable Diffusion).</b>
      </h1>                 
    </div>
  </div>


  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Image Grounded T2I Generation (Bounding box)</h2>
      <img id="teaser" width="95%" src="images/image_condition.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN can also ground on reference images. Top row indicates reference images can provide more fine-grained details beyond text description such as style and shape or car. The second row shows reference image can also be used as style image in which case we find ground it into corner or edge of an image is sufficient.</b>
      </h1>                 
    </div>
  </div>


  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Grounded T2I Generation (Keypoints)</h2>
      <img id="teaser" width="95%" src="images/keypoint.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN can also ground human keypoints while doing text-to-image generation.</b>
      </h1>                 
    </div>
  </div>


  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Grounded Inpainting</h2>
      <img id="teaser" width="95%" src="images/inpaint.png">
      <h1>
        <p style="font-family:Times New Roman"><b>Like other diffusion models, GLIGEN can also perform grounded image inpaint, which can generate objects tightly following provided bounding boxes.</b>
      </h1>                 
    </div>
  </div>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{li2023gligen,
  author      = {Li, Yuheng and Liu, Haotian and Wu, Qingyang and Mu, Fangzhou and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan and Lee, Yong Jae},
  title       = {GLIGEN: Open-Set Grounded Text-to-Image Generation},
  publisher   = {arXiv:2301.07093},
  year        = {2023},
}
</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a
      href="https://x-decoder-vl.github.io">X-Decoder</a>, licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
    <p>

    <a href='https://github.com/Computer-Vision-in-the-Wild/'><img id="painting_icon" width="10%" src="https://avatars.githubusercontent.com/u/97258247?s=200&v=4"> 
    </a> 
    <b> Related Links</b> : 
    <div class="content has-text-justified">
      <ul>
        <li><a href='https://github.com/Computer-Vision-in-the-Wild/'>[Computer Vision in the Wild] </a> </li>
        <li>GLIGEN: (box, concept) &#8594 image || GLIP : image &#8594 (box, concept); See grounded image understanding in <a href='https://github.com/microsoft/GLIP'>[GLIP]</a></li>
        <li>Mudulated design and training of foundation models for image understanding <a href='https://react-vl.github.io/'>[REACT]</a></li>
      </ul>
 
    </div>     

    
    </p>
  </div>
</section>


</body>
</html>
